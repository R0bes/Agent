# üê≥ Agent mit Docker Compose und Ollama

Dieser Guide erkl√§rt, wie der Agent mit Docker Compose gestartet wird und dabei das GPT-OSS:20B Ollama Modell verwendet.

## üöÄ Schnellstart

### Option 1: Ollama als Docker Service (Empfohlen)

```bash
# Alle Services starten (inkl. Ollama)
docker-compose up -d

# Status √ºberpr√ºfen
docker-compose ps

# Logs anzeigen
docker-compose logs -f agent-core
```

### Option 2: Host-Ollama verwenden (Entwicklung)

```bash
# Nur Agent starten (verwendet Host-Ollama)
docker-compose -f docker-compose.yml -f docker-compose.override.yml up -d agent-core

# Status √ºberpr√ºfen
docker-compose ps
```

## üìã Services

### Ollama Service
- **Image**: `ollama/ollama:latest`
- **Port**: 11434
- **Volume**: `ollama_data` (persistente Modell-Speicherung)
- **Model**: `gpt-oss:20b` (wird automatisch heruntergeladen)

### Agent Core
- **Port**: 8000
- **LLM Provider**: Ollama
- **Model**: gpt-oss:20b
- **Abh√§ngigkeit**: Ollama Service

### Weitere Services
- **UI**: Port 4001
- **WhatsApp Bot**: Port 8001
- **Telegram Bot**: Port 8002
- **Chrome**: Port 4444 (Selenium)
- **Nginx**: Port 80/443

## üîß Konfiguration

### Umgebungsvariablen

```yaml
environment:
  - LLM_PROVIDER=ollama
  - LLM_BASE_URL=http://ollama:11434  # Docker Service
  - LLM_MODEL=gpt-oss:20b
```

### F√ºr Host-Ollama (Entwicklung)

```yaml
environment:
  - LLM_BASE_URL=http://host.docker.internal:11434
extra_hosts:
  - "host.docker.internal:host-gateway"
```

## üìä Monitoring

### Service-Status

```bash
# Alle Services
docker-compose ps

# Spezifischer Service
docker-compose ps agent-core

# Service-Logs
docker-compose logs -f ollama
docker-compose logs -f agent-core
```

### Ollama-Status

```bash
# Modell-Liste
curl http://localhost:11434/api/tags

# Server-Status
curl http://localhost:11434/api/version
```

## üß™ Testing

### Agent testen

```bash
# API-Endpunkt testen
curl -X POST http://localhost:8000/ask \
  -H "Content-Type: application/json" \
  -d '{"query": "Was ist 2+2?"}'
```

### LLM-Provider testen

```bash
# Im Container testen
docker exec -it core python test_llm_providers.py

# Oder direkt
docker exec -it core python -c "
from agent.core.core import Core
import asyncio
core = Core()
result = asyncio.run(core.ask('Hallo!'))
print(result)
"
```

## üîÑ Entwicklung

### Code-√Ñnderungen

```bash
# Code wird automatisch neu geladen (Volume-Mount)
# Einfach Dateien bearbeiten und Container neu starten

# Container neu starten
docker-compose restart agent-core

# Oder neu bauen
docker-compose up -d --build agent-core
```

### Debug-Modus

```bash
# Debug-Logs aktivieren
docker-compose up -d --build agent-core
docker-compose logs -f agent-core
```

## üö® Troubleshooting

### H√§ufige Probleme

#### 1. Ollama startet nicht
```bash
# Logs √ºberpr√ºfen
docker-compose logs ollama

# Container neu starten
docker-compose restart ollama
```

#### 2. Agent kann Ollama nicht erreichen
```bash
# Netzwerk-Status
docker network ls
docker network inspect agent-network

# Ollama-Container erreichen
docker exec -it core ping ollama
```

#### 3. Modell nicht verf√ºgbar
```bash
# Modell manuell herunterladen
docker exec -it ollama ollama pull gpt-oss:20b

# Modell-Liste
docker exec -it ollama ollama list
```

### Logs analysieren

```bash
# Alle Logs
docker-compose logs

# Spezifische Fehler
docker-compose logs agent-core | grep ERROR
docker-compose logs ollama | grep ERROR
```

## üìà Performance

### Ressourcen-Monitoring

```bash
# Container-Ressourcen
docker stats

# Spezifischer Container
docker stats ollama
docker stats agent-core
```

### Optimierungen

- **Ollama**: GPU-Support aktivieren (falls verf√ºgbar)
- **Agent**: Memory-Limits anpassen
- **Netzwerk**: Inter-Container-Kommunikation optimieren

## üîê Sicherheit

### Netzwerk-Isolation

- Alle Services laufen im `agent-network`
- Ollama ist nur intern erreichbar
- Externe Ports sind minimiert

### Volume-Sicherheit

- Ollama-Daten sind persistent
- Code-Volumes sind read-only (au√üer Entwicklung)

## üìö Weitere Informationen

- [Ollama Docker Hub](https://hub.docker.com/r/ollama/ollama)
- [Docker Compose Dokumentation](https://docs.docker.com/compose/)
- [Agent LLM Engine](../agent/llm_engine/README.md)
